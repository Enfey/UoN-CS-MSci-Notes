The core of all language models (RNNs, Transformers, etc) is a simple task:
> **Predict the next token given the previous ones**

Formally:
$$P(w_{t+1} ∣ w_{1}​,w_{2}​,\dots, w_{t})$$
Everything is constructed atop this notion. 

## Training an RNN/Language model
- Train the RNN to generate the next token
	- Model error via cross-entropy loss as discussed prior.
- Allow for large scale **self-supervised learning**
	- SSL uses **pretext tasks** to synthesise pseudo-labels from unlabelled data
	- The model learns to predict parts of its input, such as a missing word; the label comes from the data itself. 
	- The model is then fine tuned for downstream tasks using supervised learning
- This is a key insight supporting the creation of **foundation models**
	- Large machine learning models trained on 'general' tasks, to later be tightly specialised via fine-tuning

## Training an LLM
- LLMs are pretrained in an **unsupervised** manner
	Huge corpora such as:
	- The pile (825GB, including web, papers, books, code etc)
	- Common Crawl (~20B webpages)
	- Books, papers, code, forums.
	Are used in pretraining. 
- During pretraining, the model learns grammar, syntax, word meanings, facts and patterns, reasoning structure, and how language flows via the prescription of pretext tasks. 
### Training phases of LLMs
#### Unsupervised pre-training
- The model is fed text and trained to predict the next token in a sequence. This teaches the model fundamental **language structure**, **grammar**, and common **word associations**.
	- Pre-training often requires a lot of computational resources and can take weeks or even months, even with powerful hardware.
- We take the same loss function: cross-entropy loss
	- We want the model to assign a high probability to true word $w$ = we want loss to be **high** if the model assigns too low a probability to $w$ 
	- CE loss is the **negative log probability** that the model assigns to the true next word $w$.
	- We move the model weights in a direction that assigns a higher probability to $w$ via the model optimiser, after backpropagation. 
- **Teacher forcing an LLM**
	- During training, the model **always receives the true previous word** for future probability computations, not its own prediction. Ensures early mistakes don't compound easily, and the model will not drift from correct context; enforce stable training by ensuring that we stay close to ground truth. Simply ignore prior predictions. 

#### Fine-tuning
- Supervised fine-tuning
	- The pretrained model is refined for specific tasks:
		- Text summarisation
		- Question Answering
		- Translation
		- Reasoning
- Fine-tuning helps the model learn the nuances and specific requirements of the task at hand
	- Fine-tuning on the same task (next token prediction/masked language modelling) but on a more specialised dataset (continued pretraining task)
	- Fine-tuning on a downstream task closer to real world usage.

##### RLHF (Reinforcement Learning with Human Feedback)#
1. **Reward model training:**
	1. **Human feedback**: Humans rate and compare several different responses generated by the language model. This data conveys which kinds of outputs are seen as better. 
	2. **Reward model**: A separate model is trained to predict human preferences based on the human feedback data. This model learns to assign a reward score to a given piece of text. 
2. **Policy optimisation using RL**
	