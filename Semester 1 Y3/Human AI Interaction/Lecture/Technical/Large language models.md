The core of all language models (RNNs, Transformers, etc) is a simple task:
> **Predict the next token given the previous ones**

Formally:
$$P(w_{t+1} ∣ w_{1}​,w_{2}​,\dots, w_{t})$$
Everything is constructed atop this notion. 

## Training an RNN/Language model
- Train the RNN to generate the next token
	- Model error via cross-entropy loss as discussed prior.
- Allow for large scale **self-supervised learning**
	- SSL uses **pretext tasks** to synthesise pseudo-labels from unlabelled data
	- The model learns to predict parts of its input, such as a missing word; the label comes from the data itself. 
	- The model is then fine tuned for downstream tasks using supervised learning
- This is a key insight supporting the creation of **foundation models**
	- Large machine learning models trained on 'general' tasks, to later be tightly specialised via fine-tuning

## Training an LLM
- LLMs are pretrained in an **unsupervised** manner
	Huge corpora such as:
	- The pile (825GB, including web, papers, books, code etc)
	- Common Crawl (~20B webpages)
	- Books, papers, code, forums.
	Are used in pretraining. 
- During pretraining, the model learns grammar, syntax, word meanings, facts and patterns, reasoning structure, and how language flows via the prescription of pretext tasks. 
### Training phases of LLMs
#### Unsupervised pre-training
- The model is fed text and trained to predict the next token in a sequence. This teaches the model fundamental **language structure**, **grammar**, and common **word associations**.
	- Pre-training often requires a lot of computational resources and can take weeks or even months, even with powerful hardware.
- We take the same loss function: cross-entropy loss
	- We want the model to assign a high probability to true word $w$ = we want loss to be **high** if the model assigns too low a probability to $w$ 
	- CE loss is the **negative log probability** that the model assigns to the true next word $w$.
	- We move the model weights in a direction that assigns a higher probability to $w$ via the model optimiser, after backpropagation. 
- **Teacher forcing an LLM**
	- During training, the model **always receives the true previous word** for future probability computations, not its own prediction. Ensures early mistakes don't compound easily, and the model will not drift from correct context; enforce stable training by ensuring that we stay close to ground truth. Simply ignore prior predictions. 

#### Fine-tuning
- Supervised fine-tuning
	- The pretrained model is refined for specific tasks:
		- Text summarisation
		- Question Answering
		- Translation
		- Reasoning
- Fine-tuning helps the model learn the nuances and specific requirements of the task at hand
	- Fine-tuning on the same task (next token prediction/masked language modelling) but on a more specialised dataset (continued pretraining task)
	- Fine-tuning on a downstream task closer to real world usage.

##### RLHF (Reinforcement Learning with Human Feedback)#
1. **Reward model training:**
	1. **Human feedback**: Humans rate and compare several different responses generated by the language model. This data conveys which kinds of outputs are seen as better. 
	2. **Reward model**: A separate model is trained to predict human preferences based on the human feedback data. This model learns to assign a reward score to a given piece of text. 
2. **Policy optimisation using RL**
	1. **Fine-tuning the LLM**: The original language model is further fine-tuned, but instead of directly using a task-specific reward function, it uses the reward scores from the trained reward model. 
	2. **Reinforcement learning loop**: The updated model generates new text, and the reward model again provides feedback, guiding the LLM to adjust its weights to increase future reward, thus producing output more in line with what humans would consider desirable.

#### Decoding: How the model generates text
- The task of choosing a word to generate based on the model's probabilities is called **decoding.**
- The model outputs a probability distribution over tokens at each token. 
- The most common method for decoding in LLMs is **sampling**, sample from a model's distribution over words according to their probability assigned by the model.
##### Greedy decoding
> Picks the most probable word every time.

Leads to boring, repetitive text.

##### Top-K sampling
1. Choose $\#$ of words $k$ 
2. For each $w \in V$, use the language model to compute the likelihood of this word given the context $P(w_t \ | \ w_{<t})$ 
3. Sort the words by likelihood, keep only the top $k$ most probable words.
4. Normalise the scores of the $k$ words to be a legitimate probability probability distribution. 
5. Randomly sample a word from within these remaining $k$ most-probable words according to its probability
Fixed $k$ can be too restrictive or too broad depending on situation. 

##### Top-p sampling
- Instead of a fixed number of words:
	- Select the smallest set of words whose total probability $\ge p$ 
- That is, keep top $p$ percent of the probability mass
- Given a distribution $P(w_t \ | \ w_{<t})$ the top-$p$ vocabulary $V^{(p)}$ is the smallest set of words such that: $$\sum_{w \in V^{(P)}}P(w_{t} \ | \ w_{<t})\ge p$$
- Enforces dynamic adaptation. 

##### Temperature sampling
- Intuition from thermodynamics. 
	- A system at high temperature is flexible and can explore many possible configurations/states
	- A system at a lower temperature is likely to explore a subset of lower energy states
- Applies a scaling factor $\tau$ to the model's raw output scores before they are squashed into a probability distribution via softmax:
$$P_i = \frac{e^\frac{{z_{i}}}{T}} {\sum_{j}e^\frac{z_{j}}{T}}$$
- **Low-temperature sampling** $\tau < 1$ 
	- Sharpens the probability distribution, increasing likelihood of high-probability words, leading to more predictable, factual, and coherent text, closer to greedy encoding.
- **High-temperature sampling** $\tau > 1$ 
	- Flattens the distribution, giving lower probability words a better chance, resulting in more diverse, creative, sometimes nonsensical/hallucinatory output.
- $\tau = 1$ uses original probabilities.
### Challenges in LLMs
- LLMs seem to "lie" and "hallucinate" i.e., give factually-incorrect responses to questions
- This happens because the model must **always** generate something, and it is sampling from probabilities, not verifying truth. The training data may also contain errors.
- There are a few ways to alleviate the issue:
	- **In-context learning**: teaching the LLM at inference time rather than at training time - task demonstrations are integrated into the prompt in a natural language format. Allows pre-trained LLMs to address new tasks without fine tuning the model, but accumulated knowledge is transient as the probability distribution is merely being biased. 
	- **Reasoning**: forcing the LLM to think in a structured way, generating intermediate steps before answering. Allows for problem decomposition; the model 'thinks' more at inference time.
	- **Grounding**: forcing the LLM to base its answers on **real, external** information, not just its internal knowledge. E.g., **RAG**.
- The output of an LLM is determined by both training data, and what information it receives from the operator; **prompt engineering** means tailoring questions and input to control the probability distribution. 
	- Advanced prompting techniques include:
		- **Chain-of-thought prompting**
			- Asking model to show reasoning step by step before giving final answer to decompose problems and reduce reasoning errors by forcing ex
		- **Directional Stimulus prompting**
			- Guiding the model toward a specific reasoning *direction/strategy* without explicitly giving steps
			- E.g., "Think carefully about cause and effect", "Consider edge cases before answering"
			- Nudges model toward certain internal reasoning patterns without fully spelling them out. 
		- **Meta prompting**
			- Prompting the model about how it should **think or behave**, rather than what content to generate.
			- "You are an expert mathematician"
		- **Tree of Thoughts**
			- Instead of single chain of reasoning, model explores **multiple possible reasoning paths** and evaluates them. 
				1. Generate possible steps/states
				2. Evaluate each step
				3. Expand the best ones
				4. Prune weak paths
		- **Self-consistency prompting**
			- Model generates multiple independent reasoning chains and then picks the most common final answer. 
			- E.g., sample 5 chain of thought answers, extract final answers, then choose the most frequent one. 
#### Grounding in LLMs: Retrieval-Augmented Generation
- Instead of forcing LLM to memorise everything and increasing likelihood of hallucination, we give access to external knowledge at inference time. 
- We can retrieve and artificially inject relevant facts into the prompt itself. 
- **RAG pipeline:**
	1. User asks a question or the LLM generates a query from the prompt
	2. Convert question to an embedding
	3. Search vector database (FAISS, Milvus, etc.)
	4. Retrieve top-$k$ relevant chunks
	5. Inject them into the prompt
	6. LLM generates an answer grounded in retrieved text. 
	![](Pasted%20image%2020251227021542.png)
##### Graph RAG
- Standard RAG treats knowledge as **chunks of text** but many domains are **relational** and need finer granularity, or need to be able to down reasoning on your knowledge e.g., joining concepts from different documents. 
- **Knowledge graphs** can be used and queried by LLMs; instead of top-$k$ documents we traverse a graph of entities and relationships. 
- Better for modelling complex relationships and capturing structure rather than just similarity. 
	Example Query:
	 “Which drugs target proteins related to Alzheimer’s?”
	Graph traversal:
	- Alzheimer’s → related proteins → associated drugs
	This is difficult for text-only RAG.

### Recent trends in LLMs
#### Multimodal LLMs
- Multimodal LLMs refer to LLMs that understand multiple data types like text, images, audio, and video, transcending traditional text-only models.
- Multimodal system typically has an image encoder, audio encoder, and a text encoder which take raw data and transform into rich, numerical embeddings. These are then aligned onto a shared embedding space, the same one as the LLM's text tokens to make the modalities directly combinable. which convert their respective representations into a shared embedding space
#### Quantisation
- Technique to shrink LLMs by reducing the precision of their weights and activations to cut memory usage and speed up inference. 
- LLMs are huge, with billions of parameters, and are very expensive to run. 
- Techniques such as **LoRA/QLoRA** try to find a smaller model that performs simililarly

#### Specialised LLMs
- Rather than one general model, construct:
	- Medical LLMs
	- Legal LLMs
	- Code LLMs
- These are more reliable in-domain. 
#### Agentic Architectures
- An agentic LLM is an LLM composed of several modular layers mimicking cognitive processes to mark itself as an autonomous system capable of reasoning, planning, deciding actions, calling tools, observing results, and iterating, to achieve high-level goals. 
	![](Pasted%20image%2020251227023524.png)
	![](Pasted%20image%2020251227023543.png)
	

#### Tool calling
- Core mechanism that permits LLMs to interact externally by invoking APIs, databases, and custom code; permits capability of performing real-world, effective tasks. 
	![](Pasted%20image%2020251227024147.png)
- Output is fed back into model, and the model can then decide the next step
- Examples of tools:
	- Web search
	- Code execution
	- File access
	- Database queries
	- Browser control. 
- Example JSON schema output:
	```json
	{
	  "tool": "search_web",
	  "arguments": {"query": "latest inflation rate"}
	}
	```
#### Mixture-of-Experts models
- Instead of one dense model, maintain many sparsely connected  'expert' networks with identical architecture 
- Gating network/mechanism routes the input to the experts (weighted sum, of top-$k$ experts)
	- Token embedding goes into router/gate
	- Suppose have $n$ experts, router outputs $r = [r_1, r_2, ..., r_N]$
	- Choose top $k$ experts with highest scores
	- Compute weighted sum of expert outputs
		- Each expert produces an output vector $y_i = Expert_i(x)$ 
		- The final output is: $$y = \sum_{i \in top-k} w_{i} \cdot y_{i} $$
			Where $w_i$ are normalised routing weights via softmax, 
- A small subset of experts is used at all times: faster inference
- This results in much lower compute cost. 
### Evaluating and Benchmarking LLMs
- Like each field, LLMs have their own way of measuring progress
- Advantages:
	- Track progress, identify areas for improvement (future work)
	- Identify strengths/weaknesses (to help select the correct model)
- Drawbacks
	- Most evaluations oversimplify considerably; they are reductionist
	- Models can overfit benchmarks
	- Not ecologically valid
#### Evaluation Dimensions
1. **Accuracy and Fluency**
	Grammaticality, style, writing level; are outputs correct and natural?
2. **Reasoning**
	Logical reasoning ability; can it follow logic and solve multi-step problems?
3. **Knowledge and Common Sense**
	Factual knowledge, general knowledge, commonsense inference.
4. **Bias and Fairness**
	Social biases, racial biases
5. **Efficiency and Scalability**
	Computational resources needed, inference speed.

