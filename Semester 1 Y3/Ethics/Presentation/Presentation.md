giI stand mostly for the proposition. When negative outcomes arise from the use of generative AI, primary responsibility should rest with the user/deployer, while creators retain enforceable duties to design safely and disclose risks.

By negative outcomes, I refer to harms such as privacy breaches, defamation, discrimination, facilitation of illegal acts, or real-world harms from decisions based on model outputs. The user/deployer is the one that prompts, acts on, or publishes outputs. We term the creator/provider as being the developer(s) that design, train, and supply the model and user-facing service.

Moral responsibility classically follows agency. A user who chooses a prompt or amplifies an output exercises proximate control over a potential harmful action to be taken; the creator does not make that final choice. Thus from a deontological perspective, the duty to not harm attaches to the acting agent. From utilitarian angle however, assigning primary responsibility to deployers would yield maximum benefit as it would create root incentives for verification, oversight, and guardrails.

The EU AI act explicitly assigns duties to deployers of high-risk systems: Article 26(1) requires deployers to take appropriate technical and organisational measures and to use systems "in accordance with instructions for use" and to implement human oversight and comptetent staff. At the same time, the Act imposes obligations on providers of generative models, mandating technical documentation, model evaluation, adversarial testing, and post-market monitoring in line with Articles 53 through 55. These dual duties show the legislature's allocation; creators must reduce structural risk and be transparent from the get-go, and deployers must operate responsibly or face motivated consequences.

It can be said that creators cannot disclaim all responsibility. Professional standards require diligence; the ACM code of Ethics Clause 2.5 instructs computing professionals to give "comprehensive and thorough evaluations of computer systems and their impacts, including analysis of possible risks". Creators therefore have a deontological and professional obligation to evaluate, mitigate, and disclose foreseeable risks and the failure to do so is both ethically and legally blameworthy.

The fairest, most practically effective rule is shared responsibility: the primary liability for harms caused by deployment decisions sits with the users; liability for harms caused by defective and inherent design, failure to adequately test, and withheld disclosure rests with creators. This mirrors the AI Act's split duties and aligns with product and data legislation. This is seen in UK GDPR Article 22, which restricts certain automated decision making practices, and Article 82, which provides remedies for damage, these are further legal mechanisms that support accountability along prior discussed role lines. 

Opponents of my position postulate that creators should bear primary responsibility because harms are foreseeable and designers control core behaviour. This is strong; creators must foresee and mitigate systemic risks. However, real-world cases show the limits of creator foresight. Microsoft's Tay chatbot 2016 was manipulated shortly after its 2016 deployment by co-ordinated user prompting, eliciting racist content generation, demonstrating how even well-tested systems can be hijacked by dedicated users. 

Conversely, Amazon's experimental recruiting model which was scrapped in 2018 revealed embedded bias against women due to flawed training data, an example of a design-stage failure that creators must remedy via methods like data augmentation and synthesis, and sufficient capability disclosure . 

These contrasting cases reinforce that both sides can cause harm, therefore neither exclusive creator responsibility nor exclusive user responsibility is defensible. But they also show that misuse often arises directly from human prompting and deployment decisions, not from creator intention alone. Holding creators solely responsible would blunt incentives for cautious deployment, and potentially stifle innovation, simple via unfairly punishing providers according to how users willfully misuse outputs. 

When harm occurs, we must ask two factual questions. 1) Did the provider meet legal and professional obligations, that is, sufficient testing, documentation, transparency, and mitigation? If not, the provider bears primary responsibility. 2) Did the deployer follow creator instructions, apply reasonable oversight, and exercise both moral and professional judgement? If not, the deployer bears primary responsiblity. This aligns with the AI act's dual regime and with professional ethics in computer science. 

I declare that creators must design responsibly, disclose risks and correct systemic defects, but the decisive moral agency of choosing to publish, amplify, act on, mangle, or rely on model output, lies with the user, especially in a world where complex professional and legal obligations ensure that providers deliver the safest generative systems possible. Therefore, assigning primary responsibility to users, whilst holding creators accountable for design failures, best balances fairness, deterrence, and innovation, and this argument is supported by both legislation and theoretical ethics. For ethical clarity and practical risk reduction, this is the most  the most justifiable and effective allocation of responsibility.

